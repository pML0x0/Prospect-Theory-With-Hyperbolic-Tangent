{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from Environments.Stochastic_GridWorld8Actions import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(rewards_across_seeds, traps_across_seeds, variance_across_seeds):\n",
    "\n",
    "    kernel_size = 11000\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "\n",
    "    all_rewards = []\n",
    "    for data in rewards_across_seeds:\n",
    "        rewards = np.convolve(data, kernel)\n",
    "        all_rewards.append(rewards[kernel_size:-kernel_size])\n",
    "\n",
    "    all_traps = []\n",
    "    for data in traps_across_seeds:\n",
    "        traps = data\n",
    "        all_traps.append(traps)\n",
    "\n",
    "    return all_rewards, all_traps#, all_variance_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_variance = []\n",
    "\n",
    "file = open('test_transitions_for_variance_1.0.pickle', 'rb')\n",
    "# dump information to that file\n",
    "transition = pickle.load(file)\n",
    "transitions_variance.append(transition)\n",
    "# close the file\n",
    "file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.3.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.5.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.7.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta_data in [0,0.2,0.3,0.4,0.6,0.8]:\n",
    "    env_obj = GridWorld(x_dim = 7, y_dim = 7, deterministic_T_prob = 1.0, reward_location=42) \n",
    "    env_obj.state_transitions = transitions_variance[0]\n",
    "\n",
    "    # env_obj.set_penalty(2, 1)\n",
    "    # env_obj.set_penalty(4, 3)\n",
    "    # env_obj.set_penalty(3, 4)\n",
    "\n",
    "    env_obj.set_penalty(2, 1)\n",
    "    env_obj.set_penalty(4, 3)\n",
    "    env_obj.set_penalty(3, 4)\n",
    "\n",
    "\n",
    "    traps_across_seeds = []\n",
    "    rewards_across_seeds = []\n",
    "    std_across_seeds = []\n",
    "\n",
    "    trajectory_heat_map_arr = []\n",
    "    q_values_across_seeds = []\n",
    "\n",
    "    for seed in range(4):\n",
    "\n",
    "        EPISODES = 800000\n",
    "        \n",
    "        traps = np.zeros(EPISODES)\n",
    "        rewards_arr_episodes = []\n",
    "        std_per_episode = []\n",
    "\n",
    "        agent_obj = Table_Q_Learning(lr = 0.01, state_space=49, annealing_coefficient =  0.999999,\n",
    "                                      beta = beta_data, seed=seed)\n",
    "        trajectory_heat_map = np.zeros((7,7))\n",
    "        q_values_arr = []\n",
    "        \n",
    "        for i in range(EPISODES): #episodes\n",
    "        \n",
    "            state = env_obj.reset()[0]\n",
    "            mean_episodic_reward = 0\n",
    "            #variance_episodic_reward = 0\n",
    "            #std_episodic_reward = 0\n",
    "            \n",
    "            for j in range(1,5001): #steps    \n",
    "\n",
    "                action, q, all_q = agent_obj.take_action( state ) \n",
    "                new_state, reward, done, _ = env_obj.step(state, action)\n",
    "                new_state = new_state[0]\n",
    "                \n",
    "                tempx = (reward - mean_episodic_reward)\n",
    "                mean_episodic_reward += tempx / j\n",
    "\n",
    "\n",
    "                agent_obj.update(state, action, new_state, reward, done, env_obj)\n",
    "\n",
    "\n",
    "               \n",
    "                x, y = np.where(env_obj.grid == state)\n",
    "                trajectory_heat_map[x[0]][y[0]] +=1\n",
    "            \n",
    "                \n",
    "                if(done == 1):\n",
    "                    \n",
    "                    x, y = np.where(env_obj.grid == new_state)\n",
    "                    trajectory_heat_map[x[0]][y[0]] +=1\n",
    "\n",
    "                     \n",
    "                    # if i % 1000 == 0 and i > 140000:\n",
    "                    #     print(\"Episode \",i, \" Epsilon \",agent_obj.epsilon, \" mean reward \",mean_episodic_reward)\n",
    "                    if(env_obj.state_attributes[new_state] == 2):\n",
    "                        traps[i] = 1\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "        \n",
    "\n",
    "            rewards_arr_episodes.append(mean_episodic_reward)\n",
    "            #q_values_arr.append(agent_obj.q_table)\n",
    "            #std_per_episode.append(std_episodic_reward)\n",
    "\n",
    "        traps_across_seeds.append(traps)\n",
    "        rewards_across_seeds.append(rewards_arr_episodes)\n",
    "        std_across_seeds.append(std_per_episode)\n",
    "        trajectory_heat_map_arr.append(trajectory_heat_map)\n",
    "        q_values_across_seeds.append(agent_obj.q_table)\n",
    "\n",
    "    all_rewards, all_traps = smoothing_data(rewards_across_seeds, traps_across_seeds, std_across_seeds)\n",
    "    # with open('Prospect_framework_reward_zeta_test_'+str(zeta_data)+'.pickle', 'wb') as file:\n",
    "    #     pickle.dump(all_rewards, file)\n",
    "\n",
    "    with open('Prospect_framework_traps_zeta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(all_traps, file)\n",
    "\n",
    "    # # with open('Prospect_framework_variance_'+determinism+'.pickle', 'wb') as file:\n",
    "    # #     pickle.dump(all_variance_rewards, file)\n",
    "\n",
    "    with open('Prospect_framework_heatmap_beta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(trajectory_heat_map_arr, file)\n",
    "    \n",
    "    with open('Prospect_framework_q_value_beta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(q_values_across_seeds, file)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renv2-safety",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
