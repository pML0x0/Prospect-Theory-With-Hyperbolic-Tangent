{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from Environments.Stochastic_GridWorld8Actions import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_data(rewards_across_seeds, traps_across_seeds, variance_across_seeds):\n",
    "\n",
    "    kernel_size = 11000\n",
    "    kernel = np.ones(kernel_size) / kernel_size\n",
    "\n",
    "    all_rewards = []\n",
    "    for data in rewards_across_seeds:\n",
    "        rewards = np.convolve(data, kernel)\n",
    "        all_rewards.append(rewards[kernel_size:-kernel_size])\n",
    "\n",
    "    all_traps = []\n",
    "    for data in traps_across_seeds:\n",
    "        traps = data\n",
    "        all_traps.append(traps)\n",
    "\n",
    "\n",
    "    return all_rewards, all_traps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table_Q_Learning():\n",
    "    def __init__(self, action_space = 8, state_space = 100, lr = 0.0001, gamma = 0.99, epsilon = 0.91, \n",
    "                 annealing_coefficient =  0.9999999, beta = 0, seed = 0):\n",
    "        self.rng = np.random.default_rng( seed )\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "\n",
    "        self.possible_actions = np.arange(action_space)\n",
    "\n",
    "        self.lr = lr #learning rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.annealing_coefficient = annealing_coefficient\n",
    "        \n",
    "        self.beta = beta\n",
    "\n",
    "        self.q_table = [\n",
    "                        [0.0] * action_space\n",
    "                        for _ in range(state_space)\n",
    "                        ] \n",
    "\n",
    "        self.pi_table = [\n",
    "                        [1.0/action_space] * action_space\n",
    "                        for _ in range(state_space)\n",
    "                        ]\n",
    "                    \n",
    "    def update(self, current_state, action, next_state, reward, done, env_obj):    \n",
    "        \n",
    "        max_next_q = np.max(self.q_table[next_state])\n",
    "        n_max_next_q = 0\n",
    "        for q in self.q_table[next_state]:\n",
    "            if(q == max_next_q):\n",
    "                n_max_next_q +=1\n",
    "\n",
    "        non_greedy_actions_prob = self.epsilon / self.action_space\n",
    "        greedy_action_prob = (1-self.epsilon)/n_max_next_q + non_greedy_actions_prob\n",
    "\n",
    "        expected_q = 0\n",
    "\n",
    "        for i in range(self.action_space):\n",
    "            if(self.q_table[next_state][i] == max_next_q):\n",
    "                expected_q += greedy_action_prob * self.q_table[next_state][i]\n",
    "            else:\n",
    "                expected_q += non_greedy_actions_prob * self.q_table[next_state][i]  \n",
    "\n",
    "        td_error = reward + self.gamma * expected_q * (1-done) - self.q_table[current_state][action]\n",
    "  \n",
    "        k = self.beta\n",
    "\n",
    "        k_plus = (1 - k)\n",
    "        k_minus = (1 + k)\n",
    "\n",
    "\n",
    "        if td_error >= 0:\n",
    "            td_error = k_plus * np.tanh(1/4 * td_error )\n",
    "        else:\n",
    "            td_error = k_minus * np.tanh(1/4 * td_error )\n",
    "        \n",
    "        self.q_table[current_state][action] +=  self.lr * td_error\n",
    "\n",
    "        #annealing epsilon\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon *= self.annealing_coefficient\n",
    "\n",
    "    def take_action(self, current_state, epsilon = None):\n",
    "        ep_policy = 0\n",
    "        if epsilon == None:\n",
    "            ep_policy = self.epsilon\n",
    "        else:\n",
    "            ep_policy = epsilon\n",
    "\n",
    "        if(self.rng.random() < ep_policy):\n",
    "            random_possible_action = self.rng.choice(self.possible_actions)\n",
    "\n",
    "            q_action = self.q_table[current_state][random_possible_action]\n",
    "\n",
    "            return random_possible_action, q_action, self.q_table[current_state]\n",
    "        else:\n",
    "            max_action = np.argmax(self.q_table[current_state])\n",
    "            max_q_action = np.max(self.q_table[current_state])\n",
    "\n",
    "            return max_action, max_q_action, self.q_table[current_state]\n",
    "\n",
    "    def set_RDG_seed(self, seed):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_variance = []\n",
    "\n",
    "file = open('test_transitions_for_variance_1.0.pickle', 'rb')\n",
    "# dump information to that file\n",
    "transition = pickle.load(file)\n",
    "transitions_variance.append(transition)\n",
    "# close the file\n",
    "file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.3.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.5.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()\n",
    "\n",
    "# file = open('test_transitions_for_variance_0.7.pickle', 'rb')\n",
    "# # dump information to that file\n",
    "# transition = pickle.load(file)\n",
    "# transitions_variance.append(transition)\n",
    "# # close the file\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta_data in [0,0.2,0.3,0.4,0.6,0.8]:\n",
    "    env_obj = GridWorld(x_dim = 7, y_dim = 7, deterministic_T_prob = 1.0, reward_location=42) \n",
    "    env_obj.state_transitions = transitions_variance[0]\n",
    "\n",
    "    # env_obj.set_penalty(2, 1)\n",
    "    # env_obj.set_penalty(4, 3)\n",
    "    # env_obj.set_penalty(3, 4)\n",
    "\n",
    "    env_obj.set_penalty(2, 1)\n",
    "    env_obj.set_penalty(4, 3)\n",
    "    env_obj.set_penalty(3, 4)\n",
    "\n",
    "\n",
    "    traps_across_seeds = []\n",
    "    rewards_across_seeds = []\n",
    "    std_across_seeds = []\n",
    "\n",
    "    trajectory_heat_map_arr = []\n",
    "    q_values_across_seeds = []\n",
    "\n",
    "    for seed in range(4):\n",
    "\n",
    "        EPISODES = 800000\n",
    "        \n",
    "        traps = np.zeros(EPISODES)\n",
    "        rewards_arr_episodes = []\n",
    "        std_per_episode = []\n",
    "\n",
    "        agent_obj = Table_Q_Learning(lr = 0.01, state_space=49, annealing_coefficient =  0.999999,\n",
    "                                      beta = beta_data, seed=seed)\n",
    "        trajectory_heat_map = np.zeros((7,7))\n",
    "        q_values_arr = []\n",
    "        \n",
    "        for i in range(EPISODES): #episodes\n",
    "        \n",
    "            state = env_obj.reset()[0]\n",
    "            mean_episodic_reward = 0\n",
    "            #variance_episodic_reward = 0\n",
    "            #std_episodic_reward = 0\n",
    "            \n",
    "            for j in range(1,5001): #steps    \n",
    "\n",
    "                action, q, all_q = agent_obj.take_action( state ) \n",
    "                new_state, reward, done, _ = env_obj.step(state, action)\n",
    "                new_state = new_state[0]\n",
    "                \n",
    "                tempx = (reward - mean_episodic_reward)\n",
    "                mean_episodic_reward += tempx / j\n",
    "                # This difference is with respect to the updated mean\n",
    "                #squared_difference = (reward - mean_episodic_reward)**2\n",
    "                #variance_episodic_reward = ((j-1)*variance_episodic_reward + squared_difference) / j\n",
    "                #std_episodic_reward = np.sqrt(variance_episodic_reward)\n",
    "\n",
    "                agent_obj.update(state, action, new_state, reward, done, env_obj)\n",
    "\n",
    "\n",
    "               \n",
    "                x, y = np.where(env_obj.grid == state)\n",
    "                trajectory_heat_map[x[0]][y[0]] +=1\n",
    "            \n",
    "                \n",
    "                if(done == 1):\n",
    "                    \n",
    "                    x, y = np.where(env_obj.grid == new_state)\n",
    "                    trajectory_heat_map[x[0]][y[0]] +=1\n",
    "\n",
    "                     \n",
    "                    # if i % 1000 == 0 and i > 140000:\n",
    "                    #     print(\"Episode \",i, \" Epsilon \",agent_obj.epsilon, \" mean reward \",mean_episodic_reward)\n",
    "                    if(env_obj.state_attributes[new_state] == 2):\n",
    "                        traps[i] = 1\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "        \n",
    "\n",
    "            rewards_arr_episodes.append(mean_episodic_reward)\n",
    "            #q_values_arr.append(agent_obj.q_table)\n",
    "            #std_per_episode.append(std_episodic_reward)\n",
    "\n",
    "        traps_across_seeds.append(traps)\n",
    "        rewards_across_seeds.append(rewards_arr_episodes)\n",
    "        std_across_seeds.append(std_per_episode)\n",
    "        trajectory_heat_map_arr.append(trajectory_heat_map)\n",
    "        q_values_across_seeds.append(agent_obj.q_table)\n",
    "\n",
    "    all_rewards, all_traps = smoothing_data(rewards_across_seeds, traps_across_seeds, std_across_seeds)\n",
    "    # with open('Prospect_framework_reward_zeta_test_'+str(zeta_data)+'.pickle', 'wb') as file:\n",
    "    #     pickle.dump(all_rewards, file)\n",
    "\n",
    "    with open('Prospect_framework_traps_zeta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(all_traps, file)\n",
    "\n",
    "    # # with open('Prospect_framework_variance_'+determinism+'.pickle', 'wb') as file:\n",
    "    # #     pickle.dump(all_variance_rewards, file)\n",
    "\n",
    "    with open('Prospect_framework_heatmap_beta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(trajectory_heat_map_arr, file)\n",
    "    \n",
    "    with open('Prospect_framework_q_value_beta_test_'+str(beta_data)+'.pickle', 'wb') as file:\n",
    "        pickle.dump(q_values_across_seeds, file)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renv2-safety",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
